service/
    processor.py
    embeddings_writer.py
    archiver.py
    main.py
    utils.py
    db_writer.py
    stats_collector.py
    web_server.py
    update_headers.py
    view_web_server.py
    downloader.py
    models.py
    prepare_gpt.py
uploads/
templates/
    index.html
    results.html

----

# file: processor.py
# directory: .

import os
import time
import torch
import cv2
import shutil
import socket
import traceback
import numpy as np

from utils import configure_thread_logging, get_session_factory
from models import Batch, Image, BatchImage, BatchLog, HostLog, ImageEmbedding
from facenet_pytorch import MTCNN, InceptionResnetV1
from insightface.app import FaceAnalysis
from insightface.utils import face_align
import config  # Import the configuration module


def processing_thread(batch_queue, embeddings_queue, archive_queue, device, engine, batch_size, report_dir, stats_collector, log_level, log_output, images_without_faces_log_file, condition):
    # Set up logger for this function
    log_filename = f'logs/embedding_processor/embedding_processor_{config.MACHINE_ID}.log'
    embedding_processor_logger = configure_thread_logging('embedding_processor', log_filename, log_level, log_output)

    # Get host name
    host_name = socket.gethostname()

    # Initialize database session
    SessionFactory = get_session_factory(engine)
    session = SessionFactory()

    # Check if HostLog exists
    existing_host_log = session.query(HostLog).filter_by(host_name=host_name, function_name='embedding_processor', log_file=log_filename).first()
    if not existing_host_log:
        host_log = HostLog(host_name=host_name, function_name='embedding_processor', log_file=log_filename)
        session.add(host_log)
        session.commit()
    else:
        host_log = existing_host_log

    # Initialize MTCNN with keep_all=True
    mtcnn = MTCNN(keep_all=True, device=device)

    # Initialize InceptionResnetV1 model
    facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)

    # Initialize InsightFace model
    app = FaceAnalysis(allowed_modules=['detection', 'recognition'],
                       providers=['CUDAExecutionProvider'] if device.type == 'cuda' else ['CPUExecutionProvider'])
    app.prepare(ctx_id=0 if device.type == 'cuda' else -1, det_size=(640, 640))

    # Get the recognition model
    recognizer = app.models['recognition']

    # Target size for resizing images for MTCNN
    mtcnn_target_size = (1280, 960)  # Width x Height

    # MTCNN processing batch size
    mtcnn_batch_size = 32  # Adjust based on your GPU memory

    # Total images processed across all batches
    total_images_processed = 0

    # Function to resize image for MTCNN and compute scaling factors
    def resize_image_for_mtcnn(image, target_size):
        original_size = image.shape[:2]  # (height, width)
        h, w = original_size
        target_w, target_h = target_size

        # Calculate scaling factors
        scale_w = target_w / w
        scale_h = target_h / h

        # Resize image
        resized_image = cv2.resize(image, (target_w, target_h))

        return resized_image, scale_w, scale_h

    # Function to adjust boxes and landmarks back to original image coordinates
    def adjust_boxes_landmarks(boxes, landmarks, scale_w, scale_h):
        # Adjust boxes
        boxes[:, [0, 2]] = boxes[:, [0, 2]] / scale_w
        boxes[:, [1, 3]] = boxes[:, [1, 3]] / scale_h

        # Adjust landmarks
        landmarks[:, :, 0] = landmarks[:, :, 0] / scale_w
        landmarks[:, :, 1] = landmarks[:, :, 1] / scale_h

        return boxes, landmarks

    # Function to check if image is a valid JPEG
    def is_valid_jpeg(image_path):
        # Check if file is a valid JPEG
        try:
            with open(image_path, 'rb') as f:
                f.seek(-2, 2)
                return f.read() == b'\xff\xd9'
        except Exception as e:
            return False

    # Function to process images in batch
    def process_images_batch(image_paths, valid_image_ids, valid_filenames, valid_image_urls, logger, mtcnn_batch_size):
        embeddings_data = []
        total_images_in_batch = len(image_paths)
        total_faces = 0
        images_with_faces = 0
        images_without_faces = 0

        nonlocal total_images_processed  # To update the outer variable

        # Process images in sub-batches for MTCNN
        for batch_start in range(0, len(image_paths), mtcnn_batch_size):
            batch_end = batch_start + mtcnn_batch_size
            batch_image_paths = image_paths[batch_start:batch_end]
            batch_image_ids = valid_image_ids[batch_start:batch_end]
            batch_filenames = valid_filenames[batch_start:batch_end]
            batch_image_urls = valid_image_urls[batch_start:batch_end]

            # Read and preprocess images in the sub-batch
            images_orig = []
            images_resized = []
            scales_w = []
            scales_h = []
            valid_indices = []

            for idx, image_path in enumerate(batch_image_paths):
                img_bgr_orig = cv2.imread(image_path)
                image_id = batch_image_ids[idx]
                filename = batch_filenames[idx]
                image_url = batch_image_urls[idx]

                if img_bgr_orig is None:
                    logger.warning(f"Failed to load image: {image_path}. Removing from database.")
                    # Remove image record from database
                    try:
                        # Remove from BatchImage
                        session.query(BatchImage).filter(BatchImage.image_id == image_id).delete(synchronize_session=False)
                        # Remove from Image
                        session.query(Image).filter(Image.id == image_id).delete(synchronize_session=False)
                        session.commit()
                        logger.info(f"Removed image_id {image_id} from database.")
                    except Exception as e:
                        session.rollback()
                        logger.error(f"Error removing image_id {image_id} from database: {e}", exc_info=True)
                    continue

                # Check if image is valid JPEG
                if not is_valid_jpeg(image_path):
                    logger.warning(f"Image is not a valid JPEG: {image_path}. Removing from database.")
                    # Remove image record from database
                    try:
                        # Remove from BatchImage
                        session.query(BatchImage).filter(BatchImage.image_id == image_id).delete(synchronize_session=False)
                        # Remove from Image
                        session.query(Image).filter(Image.id == image_id).delete(synchronize_session=False)
                        session.commit()
                        logger.info(f"Removed image_id {image_id} from database.")
                    except Exception as e:
                        session.rollback()
                        logger.error(f"Error removing image_id {image_id} from database: {e}", exc_info=True)
                    continue

                # Convert original image to RGB
                img_rgb_orig = cv2.cvtColor(img_bgr_orig, cv2.COLOR_BGR2RGB)

                # Resize image for MTCNN
                img_rgb_resized, scale_w, scale_h = resize_image_for_mtcnn(img_rgb_orig, mtcnn_target_size)

                images_orig.append(img_rgb_orig)
                images_resized.append(img_rgb_resized)
                scales_w.append(scale_w)
                scales_h.append(scale_h)
                valid_indices.append(idx)

            if not images_resized:
                logger.error("No valid images to process in sub-batch.")
                continue

            # Detect faces with MTCNN in batch on resized images
            boxes_batch_resized, probs_batch_resized, landmarks_batch_resized = mtcnn.detect(images_resized, landmarks=True)

            for idx_in_subbatch, idx in enumerate(valid_indices):
                img_rgb_orig = images_orig[idx_in_subbatch]
                scale_w = scales_w[idx_in_subbatch]
                scale_h = scales_h[idx_in_subbatch]
                image_id = batch_image_ids[idx]
                filename = batch_filenames[idx]
                image_url = batch_image_urls[idx]

                boxes_resized = boxes_batch_resized[idx_in_subbatch]
                landmarks_resized = landmarks_batch_resized[idx_in_subbatch]

                total_images_processed += 1

                if boxes_resized is not None and landmarks_resized is not None:
                    num_faces = boxes_resized.shape[0]
                    total_faces += num_faces
                    images_with_faces += 1
                    stats_collector.increment_faces_found(num_faces)
                    stats_collector.increment_images_with_faces()

                    # Adjust boxes and landmarks back to original image coordinates
                    boxes_orig, landmarks_orig = adjust_boxes_landmarks(boxes_resized, landmarks_resized, scale_w, scale_h)

                    # Convert boxes to integers
                    boxes_orig = boxes_orig.astype(int)

                    # Get aligned faces for InceptionResnetV1 from original images
                    aligned_faces = mtcnn.extract(img_rgb_orig, boxes_orig, save_path=None)

                    if aligned_faces is not None and len(aligned_faces) > 0:
                        # Use aligned_faces directly without stacking
                        face_tensors = aligned_faces.pin_memory().to(device, non_blocking=True)
                        # Get embeddings from InceptionResnetV1
                        with torch.no_grad():
                            embeddings_facenet = facenet_model(face_tensors).cpu().numpy()
                        num_embeddings_facenet = embeddings_facenet.shape[0]
                        logger.debug(f'Number of embeddings from InceptionResnetV1: {num_embeddings_facenet}')

                        # Get embeddings from InsightFace
                        for i in range(num_faces):
                            # landmarks_orig[i] has shape (5, 2)
                            landmark = landmarks_orig[i]

                            # Use face_align.norm_crop to align the face on original image
                            face_aligned = face_align.norm_crop(img_rgb_orig, landmark=landmark, image_size=112)

                            # Convert the image to BGR
                            face_aligned_bgr = cv2.cvtColor(face_aligned, cv2.COLOR_RGB2BGR)

                            # Ensure the image is of type uint8
                            face_aligned_bgr = face_aligned_bgr.astype('uint8')

                            # Get embedding with recognizer
                            embedding_insight = recognizer.get_feat(face_aligned_bgr)

                            # Collect embeddings_data
                            embeddings_data.append({
                                'image_id': image_id,
                                'filename': filename,
                                'embedding': embeddings_facenet[i].tolist(),
                                'insightface_embedding': embedding_insight.tolist()
                            })
                    else:
                        logger.error("Failed to get aligned faces for InceptionResnetV1.")
                        images_without_faces += 1
                        stats_collector.increment_images_without_faces()
                        logger.info(f"No faces detected in image: {filename}")

                        # Write image URL to log file
                        images_without_faces_log_file.write(f"{image_url}\n")
                else:
                    logger.debug(f'MTCNN did not detect faces in image: {filename}')
                    images_without_faces += 1
                    stats_collector.increment_images_without_faces()
                    logger.info(f"No faces detected in image: {filename}")

                    # Write image URL to log file
                    images_without_faces_log_file.write(f"{image_url}\n")

                # Clear GPU cache every 400 images
                if total_images_processed % 400 == 0:
                    torch.cuda.empty_cache()
                    logger.info("GPU cache cleared.")

        return embeddings_data, total_images_in_batch, total_faces, images_with_faces, images_without_faces

    while True:
        batch_info = batch_queue.get()
        if batch_info is None:
            batch_queue.task_done()
            break  # Termination signal

        start_time = time.time()
        batch_id = batch_info['batch_id']
        batch_dir = batch_info['batch_dir']
        image_ids = batch_info['image_ids']
        filenames = batch_info['filenames']
        image_urls = batch_info['image_urls']
        filename_to_id = dict(zip(filenames, image_ids))
        filename_to_url = dict(zip(filenames, image_urls))

        # Check if batch is already processed
        batch = session.query(Batch).filter_by(id=batch_id).first()
        if batch.processed:
            embedding_processor_logger.info(f"Batch {batch_id} is already processed. Skipping.")
            batch_queue.task_done()
            continue

        embedding_processor_logger.info(f"Starting processing of batch {batch_id}")

        try:
            image_paths = [os.path.join(batch_dir, filename) for filename in filenames]
            valid_image_ids = []
            valid_filenames = []
            valid_image_urls = []
            valid_image_paths = []

            # Collect valid image data
            for idx, path in enumerate(image_paths):
                if os.path.isfile(path):
                    valid_image_paths.append(path)
                    valid_image_ids.append(filename_to_id[filenames[idx]])
                    valid_filenames.append(filenames[idx])
                    valid_image_urls.append(filename_to_url[filenames[idx]])
                else:
                    embedding_processor_logger.warning(f"Image file does not exist: {path}. Removing from database.")
                    # Remove image record from database
                    image_id = filename_to_id[filenames[idx]]
                    try:
                        # Remove from BatchImage
                        session.query(BatchImage).filter(BatchImage.image_id == image_id).delete(synchronize_session=False)
                        # Remove from Image
                        session.query(Image).filter(Image.id == image_id).delete(synchronize_session=False)
                        session.commit()
                        embedding_processor_logger.info(f"Removed image_id {image_id} from database.")
                    except Exception as e:
                        session.rollback()
                        embedding_processor_logger.error(f"Error removing image_id {image_id} from database: {e}", exc_info=True)

            if not valid_image_ids:
                embedding_processor_logger.warning(f"No valid images to process for batch {batch_id}. Removing batch from database.")
                # Remove batch from database
                try:
                    # Remove from BatchImage
                    session.query(BatchImage).filter(BatchImage.batch_id == batch_id).delete(synchronize_session=False)
                    # Remove batch
                    session.query(Batch).filter(Batch.id == batch_id).delete(synchronize_session=False)
                    session.commit()
                    embedding_processor_logger.info(f"Removed batch {batch_id} from database.")
                except Exception as e:
                    session.rollback()
                    embedding_processor_logger.error(f"Error removing batch {batch_id} from database: {e}", exc_info=True)
                # Delete batch directory
                shutil.rmtree(batch_dir, ignore_errors=True)
                # Decrement the counter and notify downloader
                with condition:
                    config.current_batches_on_disk -= 1
                    condition.notify()
                batch_queue.task_done()
                continue

            # Now process images in batch
            (embeddings_data, total_images_processed_in_batch, total_faces_detected,
                images_with_faces_count, images_without_faces_count) = process_images_batch(
                     valid_image_paths, valid_image_ids, valid_filenames, valid_image_urls,
                     embedding_processor_logger, mtcnn_batch_size)

            total_images = total_images_processed_in_batch
            total_faces = total_faces_detected
            images_with_faces = images_with_faces_count
            images_without_faces = images_without_faces_count

            if not embeddings_data:
                embedding_processor_logger.warning(f"No embeddings generated for batch {batch_id}. Removing batch from database.")
                # Remove batch from database
                try:
                    # Remove from BatchImage
                    session.query(BatchImage).filter(BatchImage.batch_id == batch_id).delete(synchronize_session=False)
                    # Remove batch
                    session.query(Batch).filter(Batch.id == batch_id).delete(synchronize_session=False)
                    session.commit()
                    embedding_processor_logger.info(f"Removed batch {batch_id} from database.")
                except Exception as e:
                    session.rollback()
                    embedding_processor_logger.error(f"Error removing batch {batch_id} from database: {e}", exc_info=True)
                # Delete batch directory
                shutil.rmtree(batch_dir, ignore_errors=True)
                # Decrement the counter and notify downloader
                with condition:
                    config.current_batches_on_disk -= 1
                    condition.notify()
                batch_queue.task_done()
                continue

            # Pass data for saving to database
            embeddings_queue.put((batch_id, embeddings_data))
            embedding_processor_logger.info(f"Embeddings data for batch {batch_id} added to embeddings_queue.")

            # Generate report for batch
            report = {
                'batch_id': batch_id,
                'total_images': total_images,
                'total_faces': total_faces,
                'images_with_faces': images_with_faces,
                'images_without_faces': images_without_faces
            }
            report_filename = os.path.join(report_dir, f"batch_{batch_id}_report.txt")
            with open(report_filename, 'w') as f:
                f.write(f"Batch ID: {batch_id}\n")
                f.write(f"Total images processed: {total_images}\n")
                f.write(f"Total faces detected: {total_faces}\n")
                f.write(f"Images with faces: {images_with_faces}\n")
                f.write(f"Images without faces: {images_without_faces}\n")
            embedding_processor_logger.info(f"Report for batch {batch_id} saved to {report_filename}")

            embedding_processor_logger.info(f"Batch {batch_id} processed and passed for embedding saving.")

            # Mark images as processed
            session.query(Image).filter(Image.id.in_(valid_image_ids)).update({"processed": True}, synchronize_session=False)

            # Mark batch as processed
            batch.processed = True
            session.commit()

            # Update statistics
            stats_collector.increment_images_processed(len(valid_image_ids))
            stats_collector.increment_batches_processed()

            # Associate batch with log file
            batch_log = BatchLog(batch_id=batch_id, host_log_id=host_log.id)
            session.add(batch_log)
            session.commit()

            # Remove batch directory
            shutil.rmtree(batch_dir)
            embedding_processor_logger.info(f"Removed temporary directory for batch {batch_id}")

            # Decrement the counter and notify downloader
            with condition:
                config.current_batches_on_disk -= 1
                condition.notify()

            batch_queue.task_done()
        except Exception as e:
            session.rollback()
            embedding_processor_logger.error(f"Error processing batch {batch_id}: {e}")
            embedding_processor_logger.debug(traceback.format_exc())
            batch_queue.task_done()

        stats_collector.increment_batches_processed_by_processor()
        processing_time = time.time() - start_time
        stats_collector.add_batch_processing_time('embedding_processor', processing_time)
        time.sleep(1)  # Small delay to reduce load
    session.close()

----

# file: config.py
# directory: .

MACHINE_ID = 0
current_batches_on_disk = 0
----

# file: embeddings_writer.py
# directory: .

import time
import traceback
import socket

from utils import configure_thread_logging, get_session_factory
from models import ImageEmbedding, HostLog
import config  # Импортируем модуль конфигурации


def embeddings_writer_thread(embeddings_queue, db_queue, engine, stats_collector, log_level, log_output):
    # Set up logger for this function
    log_filename = f'logs/embeddings_writer/embeddings_writer_{config.MACHINE_ID}.log'
    embeddings_writer_logger = configure_thread_logging('embeddings_writer', log_filename, log_level, log_output)

    SessionFactory = get_session_factory(engine)
    session = SessionFactory()
    host_name = socket.gethostname()

    # Check if HostLog exists
    existing_host_log = session.query(HostLog).filter_by(host_name=host_name, function_name='embeddings_writer', log_file=log_filename).first()
    if not existing_host_log:
        host_log = HostLog(host_name=host_name, function_name='embeddings_writer', log_file=log_filename)
        session.add(host_log)
        session.commit()
    else:
        host_log = existing_host_log

    while True:
        embeddings_info = embeddings_queue.get()
        embeddings_writer_logger.info("Received embeddings_info from queue.")
        if embeddings_info is None:
            embeddings_writer_logger.info("Termination signal received. Exiting embeddings_writer_thread.")
            embeddings_queue.task_done()
            break  # Termination signal

        start_time = time.time()
        batch_id, embeddings_data = embeddings_info
        embeddings_writer_logger.info(f"Starting to save embeddings for batch {batch_id}")
        try:
            if not embeddings_data:
                embeddings_writer_logger.info(f"No embeddings to save in batch {batch_id}")
                embeddings_queue.task_done()
                continue

            # Save embeddings to database in one transaction
            embeddings_objects = []
            for data in embeddings_data:
                embedding_vector = data.get('embedding')
                insightface_embedding_vector = data.get('insightface_embedding')

                if embedding_vector is None and insightface_embedding_vector is None:
                    embeddings_writer_logger.error(f"No embeddings for image_id {data['image_id']}")
                    continue

                embedding = ImageEmbedding(
                    image_id=data['image_id'],
                    filename=data['filename'],
                    embedding=embedding_vector,
                    insightface_embedding=insightface_embedding_vector
                )
                embeddings_objects.append(embedding)
            session.bulk_save_objects(embeddings_objects)
            session.commit()
            embeddings_writer_logger.info(f"Embeddings for batch {batch_id} committed to database.")

            # Update statistics
            embeddings_count = len(embeddings_objects)
            stats_collector.increment_embeddings_uploaded(embeddings_count, embedding_type='both')

            # Mark batch as processed
            db_queue.put(('mark_batch_processed', batch_id))

            embeddings_writer_logger.info(f"Embeddings for batch {batch_id} saved successfully.")
        except Exception as e:
            session.rollback()
            embeddings_writer_logger.error(f"Error saving embeddings for batch {batch_id}: {e}", exc_info=True)
        finally:
            embeddings_queue.task_done()

        processing_time = time.time() - start_time
        stats_collector.add_batch_processing_time('embeddings_writer', processing_time)
        time.sleep(1)  # Small delay to reduce load
    session.close()

----

# file: archiver.py
# directory: .

import threading
import time
import os
import shutil
import traceback
import socket

from utils import configure_thread_logging, get_session_factory
from models import ArchivedImage, HostLog
import config  # Импортируем модуль конфигурации

# Импортируем функции архивации из оригинального archiver.py
from archiver_functions import (
    archive_batch_to_s3,
    archive_batch_to_azure_blob,
    archive_batch_to_ftp,
    archive_batch_to_sftp,
)


def archiver_thread(archive_queue, engine, archive_type, archive_config, stats_collector, log_level, log_output):
    # Set up logger for this function
    log_filename = f'logs/archiver/archiver_{config.MACHINE_ID}.log'
    archiver_logger = configure_thread_logging('archiver', log_filename, log_level, log_output)

    SessionFactory = get_session_factory(engine)
    session = SessionFactory()
    host_name = socket.gethostname()

    # Check if HostLog exists
    existing_host_log = session.query(HostLog).filter_by(host_name=host_name, function_name='archiver', log_file=log_filename).first()
    if not existing_host_log:
        host_log = HostLog(host_name=host_name, function_name='archiver', log_file=log_filename)
        session.add(host_log)
        session.commit()
    else:
        host_log = existing_host_log

    while True:
        archive_info = archive_queue.get()
        if archive_info is None:
            archive_queue.task_done()
            break  # Termination signal

        start_time = time.time()
        batch_id = archive_info['batch_id']
        batch_dir = archive_info['batch_dir']
        filenames = archive_info['filenames']
        image_ids = archive_info['image_ids']
        filename_to_id = dict(zip(filenames, image_ids))

        archiver_logger.info(f"Starting archiving for batch {batch_id}")

        # Archive images
        archive_urls = archive_batch(batch_dir, batch_id, archive_type, archive_config, archiver_logger)

        # Now, store the archive_urls in the database
        try:
            for filename, archive_url in archive_urls.items():
                img_id = filename_to_id.get(filename)
                if img_id:
                    archived_image = ArchivedImage(image_id=img_id, archive_url=archive_url)
                    session.add(archived_image)
            session.commit()
            archiver_logger.info(f"Archived images for batch {batch_id} stored in database.")
        except Exception as e:
            session.rollback()
            archiver_logger.error(f"Error storing archive URLs for batch {batch_id}: {e}")

        # Remove batch directory
        try:
            shutil.rmtree(batch_dir)
            archiver_logger.info(f"Removed temporary directory for batch {batch_id}")
        except Exception as e:
            archiver_logger.error(f"Error deleting batch directory {batch_dir}: {e}")

        # Update statistics
        stats_collector.increment_batches_archived()

        processing_time = time.time() - start_time
        stats_collector.add_batch_processing_time('archiver', processing_time)

        archive_queue.task_done()
        time.sleep(1)  # Small delay to reduce load
    session.close()


def archive_batch(batch_dir, batch_id, archive_type, archive_config, logger):
    if archive_type == 's3':
        return archive_batch_to_s3(batch_dir, archive_config, logger)
    elif archive_type == 'azure':
        return archive_batch_to_azure_blob(batch_dir, archive_config, logger)
    elif archive_type == 'ftp':
        return archive_batch_to_ftp(batch_dir, archive_config, logger)
    elif archive_type == 'sftp':
        return archive_batch_to_sftp(batch_dir, archive_config, logger)
    else:
        logger.error(f"Unknown archive type: {archive_type}")
        return {}

----

# file: main.py
# directory: .

import os
import argparse
import threading
import logging
import time
import shutil

from queue import Queue
from utils import configure_thread_logging, get_engine, setup_database
from stats_collector import StatsCollector, stats_logger_thread
from downloader import downloader_thread, get_total_pages_for_query, process_page, download_image
from db_writer import db_writer_thread
from processor import processing_thread
from embeddings_writer import embeddings_writer_thread
from archiver import archiver_thread, archive_batch
import torch
from insightface.app import FaceAnalysis
import config  # Импортируем модуль конфигурации

condition = threading.Condition()

# Импортируем необходимые модели и функции
from models import (
    Checkpoint, BaseImageUrl, Image, Batch, BatchImage, ImageEmbedding, ArchivedImage, HostLog, BatchLog
)
from utils import get_session_factory, configure_thread_logging
from urllib.parse import urlparse
import cv2


def main():
    parser = argparse.ArgumentParser(description='Script for parsing, downloading, and processing images.')
    parser.add_argument('-l', '--limit', type=int, default=1, help='Number of pages to process')
    parser.add_argument('-s', '--start', type=int, default=1, help='Starting page number')
    parser.add_argument('-dt', '--download-threads', type=int, default=int(os.environ.get('DOWNLOAD_THREADS', 8)),
                        help='Number of threads for downloading images in a batch (default 8)')
    parser.add_argument('-bs', '--batch-size', type=int, default=int(os.environ.get('BATCH_SIZE', 16)),
                        help='Batch size for processing images (default 16)')
    parser.add_argument('-rd', '--report-dir', type=str, default=os.environ.get('REPORT_DIR', 'reports'),
                        help='Directory for saving reports (default "reports")')
    parser.add_argument('-si', '--stats-interval', type=int, default=int(os.environ.get('STATS_INTERVAL', 10)),
                        help='Interval in seconds for statistics logging (default 10)')
    parser.add_argument('-ll','--log-level', type=str, default=os.environ.get('LOG_LEVEL', 'INFO'),
                        help='Logging level (default INFO)')
    parser.add_argument('-lo', '--log-output', type=str, choices=['file', 'console', 'both'],
                        default=os.environ.get('LOG_OUTPUT', 'file'),
                        help='Logging output: file, console, or both (default file)')
    parser.add_argument('--loggers', type=str, default=os.environ.get('LOGGERS', ''),
                        help='Comma-separated list of loggers to include (default all)')
    parser.add_argument('--archive', action='store_true', help='Enable archiving of images after processing (default False)')
    parser.add_argument('--archive-type', type=str, choices=['s3', 'azure', 'ftp', 'sftp'], help='Type of archive storage')
    parser.add_argument('--archive-config', type=str, help='Path to archive configuration file')
    parser.add_argument('--archive-threads', type=int, default=int(os.environ.get('ARCHIVE_THREADS', 4)),
                        help='Number of archiver threads (default 4)')
    parser.add_argument('--service', action='store_true', help='Run as a web service')
    parser.add_argument('--port', type=int, default=8070, help='Port for the web service (default 8070)')

    # Добавляем новый аргумент командной строки для поисковой строки
    parser.add_argument('-q', '--query', type=str, help='Search query string')

    # Добавляем аргумент для выбора режима работы
    parser.add_argument('-m', '--mode', type=str, choices=['t', 's'], default='threaded',
                        help='Mode of operation: threaded (default) or sequential for debugging.')

    args = parser.parse_args()

    # Read environment variables
    config.MACHINE_ID = int(os.environ.get('MACHINE_ID', '0'))
    TOTAL_MACHINES = int(os.environ.get('TOTAL_MACHINES', '1'))
    DOWNLOAD_DIR = os.environ.get('DOWNLOAD_DIR', 'downloads')
    MAX_BATCHES_ON_DISK = int(os.environ.get('MAX_BATCHES_ON_DISK', '5'))
    config.current_batches_on_disk = 0  # Инициализируем переменную
    DOWNLOAD_THREADS = args.download_threads
    BATCH_SIZE = args.batch_size
    REPORT_DIR = args.report_dir
    STATS_INTERVAL = args.stats_interval
    LOG_LEVEL = getattr(logging, args.log_level.upper(), logging.INFO)
    LOG_OUTPUT = args.log_output
    LOGGERS = args.loggers.split(',') if args.loggers else []

    # Archiving settings
    archive_enabled = args.archive or os.environ.get('ARCHIVE', 'False') == 'True'
    archive_type = args.archive_type or os.environ.get('ARCHIVE_TYPE')
    archive_config_path = args.archive_config or os.environ.get('ARCHIVE_CONFIG')
    ARCHIVE_THREADS = args.archive_threads

    if archive_enabled and not archive_type:
        print("Archiving is enabled but no archive type is specified.")
        return

    if archive_enabled and not archive_config_path:
        print("Archiving is enabled but no archive configuration file is provided.")
        return

    if archive_enabled:
        # Load archive configuration
        import json
        with open(archive_config_path, 'r') as f:
            archive_config = json.load(f)
    else:
        archive_config = {}

    print(f"MACHINE_ID: {config.MACHINE_ID}, TOTAL_MACHINES: {TOTAL_MACHINES}, DOWNLOAD_DIR: {DOWNLOAD_DIR}, "
          f"MAX_BATCHES_ON_DISK: {MAX_BATCHES_ON_DISK}, DOWNLOAD_THREADS: {DOWNLOAD_THREADS}, "
          f"BATCH_SIZE: {BATCH_SIZE}, REPORT_DIR: {REPORT_DIR}, STATS_INTERVAL: {STATS_INTERVAL}, "
          f"LOG_LEVEL: {args.log_level}, LOG_OUTPUT: {LOG_OUTPUT}, LOGGERS: {LOGGERS}, "
          f"ARCHIVE_ENABLED: {archive_enabled}, ARCHIVE_TYPE: {archive_type}, ARCHIVE_THREADS: {ARCHIVE_THREADS}")

    if not os.path.exists(DOWNLOAD_DIR):
        os.makedirs(DOWNLOAD_DIR)
    if not os.path.exists(REPORT_DIR):
        os.makedirs(REPORT_DIR)

    # Set up logging for main
    log_filename = os.path.join('logs', 'main', f'main_{config.MACHINE_ID}.log')
    logger = configure_thread_logging('main', log_filename, LOG_LEVEL, LOG_OUTPUT)
    logger.info("Application started.")

    # Set up the database
    engine = get_engine()
    setup_database(engine)

    # Initialize the statistics collector
    stats_collector = StatsCollector()

    # Initialize models for extracting embeddings
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")

    # Open log file for images without faces
    log_file_path = os.path.join('logs', f'images_without_faces_{config.MACHINE_ID}.log')
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
    images_without_faces_log_file = open(log_file_path, 'a')

    # Вычисляем total_pages_to_process перед запуском потоков
    total_pages_to_process = 0  # Инициализируем переменную

    # Если задан поисковый запрос, обрабатываем его
    if args.query:
        search_query = args.query
        total_pages = get_total_pages_for_query(search_query)
        total_pages_to_process = total_pages
        logger.info(f"Total pages for query '{search_query}': {total_pages}")
    else:
        total_pages_to_process = args.limit

    mode = args.mode

    if mode == 't': # threaded
        # Task queues
        page_queue = Queue()
        batch_queue = Queue()
        embeddings_queue = Queue()
        db_queue = Queue()
        batch_ready_queue = Queue()
        archive_queue = Queue()

        # Event to stop the stats logger thread
        stop_event = threading.Event()

        # Fill the page queue for processing
        if args.query:
            for page_num in range(1, total_pages_to_process + 1):
                if (page_num % TOTAL_MACHINES) != config.MACHINE_ID:
                    continue  # This page is not for this machine

                page_info = {'page_number': page_num, 'query': search_query}
                page_queue.put(page_info)
        else:
            for page_num in range(args.start, args.start + args.limit):
                if (page_num % TOTAL_MACHINES) != config.MACHINE_ID:
                    continue  # This page is not for this machine

                page_info = {'page_number': page_num, 'query': None}
                page_queue.put(page_info)

        # Start threads
        stats_logger = threading.Thread(target=stats_logger_thread, args=(
            stats_collector,
            STATS_INTERVAL,
            stop_event,
            LOG_LEVEL,
            LOG_OUTPUT,
            total_pages_to_process,  # Передаем total_pages_to_process
            page_queue,
            batch_queue,
            embeddings_queue,
            db_queue
        ))
        db_writer = threading.Thread(target=db_writer_thread, args=(db_queue, batch_ready_queue, engine, stats_collector, LOG_LEVEL, LOG_OUTPUT))
        embeddings_writer = threading.Thread(target=embeddings_writer_thread, args=(embeddings_queue, db_queue, engine, stats_collector, LOG_LEVEL, LOG_OUTPUT))
        downloader = threading.Thread(target=downloader_thread, args=(
            page_queue, batch_queue, db_queue, batch_ready_queue, DOWNLOAD_DIR, DOWNLOAD_THREADS, stats_collector,
            LOG_LEVEL, LOG_OUTPUT, archive_enabled, condition, MAX_BATCHES_ON_DISK))

        processor = threading.Thread(target=processing_thread, args=(
            batch_queue, embeddings_queue, archive_queue, device, engine, BATCH_SIZE, REPORT_DIR, stats_collector,
            LOG_LEVEL, LOG_OUTPUT, images_without_faces_log_file, condition))

        archiver_threads = []
        if archive_enabled:
            for _ in range(ARCHIVE_THREADS):
                archiver = threading.Thread(target=archiver_thread, args=(archive_queue, engine, archive_type, archive_config, stats_collector, LOG_LEVEL, LOG_OUTPUT))
                archiver_threads.append(archiver)
                archiver.start()

        db_writer.start()
        downloader.start()
        embeddings_writer.start()
        processor.start()
        stats_logger.start()

        logger.info("All threads started.")

        # Завершаем очередь страниц
        page_queue.put(None)
        page_queue.join()

        # Wait until all batches are processed
        batch_queue.join()
        embeddings_queue.join()
        db_queue.join()

        # Terminate remaining threads
        batch_queue.put(None)
        embeddings_queue.put(None)
        db_queue.put(None)
        batch_ready_queue.put(None)

        downloader.join()
        db_writer.join()
        embeddings_writer.join()
        processor.join()

        # Close archive queue and wait for archiver threads
        if archive_enabled:
            for _ in archiver_threads:
                archive_queue.put(None)
            archive_queue.join()
            for archiver in archiver_threads:
                archiver.join()

        # Stop the stats logger thread
        stop_event.set()
        stats_logger.join()

        # Close the images_without_faces log file
        images_without_faces_log_file.close()
        logger.info("All batches processed.")
    else:
        # Sequential mode
        run_sequential(args, total_pages_to_process, DOWNLOAD_DIR, DOWNLOAD_THREADS, device, engine, BATCH_SIZE,
                       REPORT_DIR, stats_collector, LOG_LEVEL, LOG_OUTPUT, images_without_faces_log_file,
                       archive_enabled, archive_type, archive_config, condition, MAX_BATCHES_ON_DISK)

    logger.info("Application finished.")


def run_sequential(args, total_pages_to_process, DOWNLOAD_DIR, DOWNLOAD_THREADS, device, engine, BATCH_SIZE,
                   REPORT_DIR, stats_collector, LOG_LEVEL, LOG_OUTPUT, images_without_faces_log_file,
                   archive_enabled, archive_type, archive_config, condition, MAX_BATCHES_ON_DISK):
    logger = logging.getLogger('sequential')
    logger.info("Starting sequential processing mode.")

    # Создание сессии базы данных
    SessionFactory = get_session_factory(engine)
    session = SessionFactory()

    # Инициализация моделей
    app = FaceAnalysis(providers=['CUDAExecutionProvider'] if device.type == 'cuda' else ['CPUExecutionProvider'])
    app.prepare(ctx_id=0 if device.type == 'cuda' else -1)

    # Генерация списка страниц для обработки
    page_infos = generate_page_infos(args, total_pages_to_process)

    for page_info in page_infos:
        # 1. Обработка HTML страницы
        page_number = page_info['page_number']
        search_query = page_info.get('query')
        if search_query:
            page_url = f"http://camvideos.me/search/{search_query}?page={page_number}"
        else:
            page_url = f"http://camvideos.me/?page={page_number}"
        image_urls = process_page(page_url, stats_collector, LOG_LEVEL, LOG_OUTPUT)
        if not image_urls:
            logger.info(f"No images found on page {page_number}.")
            continue

        # 2. Сохранение батча в базу данных
        batch_info = store_batch_in_db(page_number, image_urls, session, LOG_LEVEL, LOG_OUTPUT)
        if not batch_info:
            logger.info(f"Batch for page {page_number} was not created.")
            continue

        # **Добавлено: Проверка лимита батчей на диске**
        with condition:
            while config.current_batches_on_disk >= MAX_BATCHES_ON_DISK:
                condition.wait()
            config.current_batches_on_disk += 1  # Увеличиваем счетчик

        try:
            # 3. Загрузка изображений
            batch_info = download_batch_images(batch_info, DOWNLOAD_DIR, DOWNLOAD_THREADS, stats_collector, LOG_LEVEL, LOG_OUTPUT, archive_enabled, session)

            # 4. Обработка эмбеддингов
            embeddings_data = process_embeddings(batch_info, app, device, stats_collector, LOG_LEVEL, LOG_OUTPUT, images_without_faces_log_file, session)

            # 5. Сохранение эмбеддингов в базу данных
            save_embeddings_to_db(batch_info['batch_id'], embeddings_data, session, stats_collector, LOG_LEVEL, LOG_OUTPUT)

            # 6. Архивация изображений (если включена)
            if archive_enabled:
                archive_batch_images(batch_info, archive_type, archive_config, session, stats_collector, LOG_LEVEL, LOG_OUTPUT)

            # 7. Обновление статистики и очистка
            logger.info(f"Completed processing for batch {batch_info['batch_id']}.")

            # Выводим статистику
            log_stats(stats_collector)
        finally:
            # **Добавлено: Уменьшаем счетчик и уведомляем**
            with condition:
                config.current_batches_on_disk -= 1
                condition.notify()

    session.close()
    logger.info("Sequential processing completed.")


def generate_page_infos(args, total_pages_to_process):
    page_infos = []
    if args.query:
        search_query = args.query
        total_pages = total_pages_to_process
        for page_num in range(1, total_pages + 1):
            if (page_num % int(os.environ.get('TOTAL_MACHINES', '1'))) != config.MACHINE_ID:
                continue
            page_info = {'page_number': page_num, 'query': search_query}
            page_infos.append(page_info)
    else:
        for page_num in range(args.start, args.start + args.limit):
            if (page_num % int(os.environ.get('TOTAL_MACHINES', '1'))) != config.MACHINE_ID:
                continue
            page_info = {'page_number': page_num, 'query': None}
            page_infos.append(page_info)
    return page_infos


def store_batch_in_db(page_number, image_urls, session, log_level, log_output):
    db_writer_logger = configure_thread_logging('db_writer_sequential', 'logs/db_writer_sequential.log', log_level, log_output)
    try:
        # Проверка существования чекпоинта
        page_url = f"http://camvideos.me/?page={page_number}"
        existing_checkpoint = session.query(Checkpoint).filter_by(page_url=page_url).first()
        if existing_checkpoint:
            db_writer_logger.info(f"Page {page_url} already processed. Skipping.")
            return None
        new_checkpoint = Checkpoint(page_url=page_url)
        session.add(new_checkpoint)
        session.commit()

        # Создание base_url
        base_urls = list(set([f"{urlparse(url).scheme}://{urlparse(url).netloc}" for url in image_urls]))
        base_url_str = base_urls[0]
        base_url = BaseImageUrl(base_url=base_url_str)
        session.add(base_url)
        session.commit()

        # Создание батча
        batch = Batch(page_number=page_number)
        session.add(batch)
        session.commit()

        # Добавление изображений
        images_data = []
        image_filenames = []
        image_paths = []  # Новое
        image_urls_list = []
        for img_url in image_urls:
            parsed_url = urlparse(img_url)
            filename = os.path.basename(parsed_url.path)
            path = os.path.dirname(parsed_url.path).lstrip('/')  # Удаляем ведущий '/'
            image = Image(base_url_id=base_url.id, path=path, filename=filename)
            images_data.append(image)
            image_filenames.append(filename)
            image_paths.append(path)
            image_urls_list.append(img_url)

        if not images_data:
            db_writer_logger.info(f"No images to insert for page {page_url}.")
            return None

        session.add_all(images_data)
        session.commit()

        # Создание batch_images
        batch_images_data = [BatchImage(batch_id=batch.id, image_id=img.id) for img in images_data]
        session.bulk_save_objects(batch_images_data)
        session.commit()

        batch_info = {
            'batch_id': batch.id,
            'batch_dir': f"batch_{batch.id}",
            'image_ids': [img.id for img in images_data],
            'filenames': image_filenames,
            'paths': image_paths,  # Добавлено
            'image_urls': image_urls_list,
        }
        db_writer_logger.info(f"Batch {batch.id} is ready.")
        return batch_info
    except Exception as e:
        session.rollback()
        db_writer_logger.error(f"Error in store_batch_in_db: {e}", exc_info=True)
        return None


def download_batch_images(batch_info, download_dir, download_threads, stats_collector, log_level, log_output, archive_enabled, session):
    downloader_logger = configure_thread_logging('downloader_sequential', 'logs/downloader_sequential.log', log_level, log_output)
    batch_id = batch_info['batch_id']
    batch_dir_name = batch_info['batch_dir']
    batch_dir = os.path.join(download_dir, batch_dir_name)
    os.makedirs(batch_dir, exist_ok=True)

    image_ids = batch_info['image_ids']
    filenames = batch_info['filenames']
    image_urls = batch_info['image_urls']

    id_to_filename = dict(zip(image_ids, filenames))
    id_to_url = dict(zip(image_ids, image_urls))

    images_to_download = []
    for img_id in image_ids:
        archived_image = session.query(ArchivedImage).filter_by(image_id=img_id).first()
        if archived_image and archive_enabled:
            archive_url = archived_image.archive_url
            local_path = os.path.join(batch_dir, id_to_filename[img_id])
            success = download_image(archive_url, local_path, downloader_logger, stats_collector)
            if not success:
                images_to_download.append(img_id)
        else:
            images_to_download.append(img_id)

    if images_to_download:
        for img_id in images_to_download:
            filename = id_to_filename[img_id]
            img_url = id_to_url[img_id]
            local_path = os.path.join(batch_dir, filename)
            success = download_image(img_url, local_path, downloader_logger, stats_collector)
            if not success:
                downloader_logger.error(f"Failed to download image {img_url}")

    batch_info['batch_dir'] = batch_dir
    return batch_info


def process_embeddings(batch_info, app, device, stats_collector, log_level, log_output, images_without_faces_log_file, session):
    embedding_processor_logger = configure_thread_logging('embedding_processor_sequential', 'logs/embedding_processor_sequential.log', log_level, log_output)
    batch_id = batch_info['batch_id']
    batch_dir = batch_info['batch_dir']
    image_ids = batch_info['image_ids']
    filenames = batch_info['filenames']
    image_urls = batch_info['image_urls']
    filename_to_id = dict(zip(filenames, image_ids))
    filename_to_url = dict(zip(filenames, image_urls))

    embeddings_data = []
    try:
        image_paths = [os.path.join(batch_dir, filename) for filename in filenames]
        valid_image_ids = []
        valid_filenames = []
        valid_image_urls = []
        images_data = []

        for idx, path in enumerate(image_paths):
            img = cv2.imread(path)
            if img is not None:
                images_data.append(img)
                valid_image_ids.append(filename_to_id[filenames[idx]])
                valid_filenames.append(filenames[idx])
                valid_image_urls.append(filename_to_url[filenames[idx]])
            else:
                embedding_processor_logger.error(f"Failed to load image: {path}")

        total_images = len(images_data)
        total_faces = 0
        images_with_faces = 0
        images_without_faces = 0

        for idx_in_batch, img in enumerate(images_data):
            faces = app.get(img)
            num_faces = len(faces)
            total_faces += num_faces
            if num_faces > 0:
                images_with_faces += 1
                stats_collector.increment_faces_found(num_faces)
                stats_collector.increment_images_with_faces()
                for face in faces:
                    embedding = face.embedding.flatten().tolist()
                    embeddings_data.append({
                        'image_id': valid_image_ids[idx_in_batch],
                        'filename': valid_filenames[idx_in_batch],
                        'embedding': embedding
                    })
            else:
                images_without_faces += 1
                stats_collector.increment_images_without_faces()
                embedding_processor_logger.info(f"No faces detected in image: {valid_filenames[idx_in_batch]}")
                images_without_faces_log_file.write(f"{valid_image_urls[idx_in_batch]}\n")

        # Обновление статистики
        stats_collector.increment_images_processed(len(valid_image_ids))
        stats_collector.increment_batches_processed()

        # Mark images as processed
        session.query(Image).filter(Image.id.in_(valid_image_ids)).update({"processed": True}, synchronize_session=False)

        # Mark batch as processed
        batch = session.query(Batch).filter_by(id=batch_id).first()
        batch.processed = True
        session.commit()

        # Удаление временной директории
        shutil.rmtree(batch_dir)
        embedding_processor_logger.info(f"Removed temporary directory for batch {batch_id}")

        return embeddings_data
    except Exception as e:
        session.rollback()
        embedding_processor_logger.error(f"Error processing embeddings for batch {batch_id}: {e}", exc_info=True)
        return []


def save_embeddings_to_db(batch_id, embeddings_data, session, stats_collector, log_level, log_output):
    embeddings_writer_logger = configure_thread_logging('embeddings_writer_sequential', 'logs/embeddings_writer_sequential.log', log_level, log_output)
    try:
        if not embeddings_data:
            embeddings_writer_logger.info(f"No embeddings to save for batch {batch_id}")
            return

        embeddings_objects = []
        for data in embeddings_data:
            embedding_vector = data['embedding']
            if len(embedding_vector) != 512:
                embeddings_writer_logger.error(f"Embedding length is not 512 for image_id {data['image_id']}")
                continue

            embedding = ImageEmbedding(
                image_id=data['image_id'],
                filename=data['filename'],
                insightface_embedding=embedding_vector
            )
            embeddings_objects.append(embedding)
        session.bulk_save_objects(embeddings_objects)
        session.commit()
        embeddings_writer_logger.info(f"Embeddings for batch {batch_id} committed to database.")
        stats_collector.increment_embeddings_uploaded(len(embeddings_objects))
    except Exception as e:
        session.rollback()
        embeddings_writer_logger.error(f"Error saving embeddings for batch {batch_id}: {e}", exc_info=True)


def archive_batch_images(batch_info, archive_type, archive_config, session, stats_collector, log_level, log_output):
    archiver_logger = configure_thread_logging('archiver_sequential', 'logs/archiver_sequential.log', log_level, log_output)
    batch_id = batch_info['batch_id']
    batch_dir = batch_info['batch_dir']
    filenames = batch_info['filenames']
    image_ids = batch_info['image_ids']
    filename_to_id = dict(zip(filenames, image_ids))

    archiver_logger.info(f"Starting archiving for batch {batch_id}")

    # Архивация изображений
    archive_urls = archive_batch(batch_dir, batch_id, archive_type, archive_config, archiver_logger)

    # Сохранение archive_urls в базе данных
    try:
        for filename, archive_url in archive_urls.items():
            img_id = filename_to_id.get(filename)
            if img_id:
                archived_image = ArchivedImage(image_id=img_id, archive_url=archive_url)
                session.add(archived_image)
        session.commit()
        archiver_logger.info(f"Archived images for batch {batch_id} stored in database.")
    except Exception as e:
        session.rollback()
        archiver_logger.error(f"Error storing archive URLs for batch {batch_id}: {e}")

    stats_collector.increment_batches_archived()


def log_stats(stats_collector):
    stats = stats_collector.reset()
    # Формируем сообщение статистики
    message = (
        f"Total Images Processed: {stats['total_images_processed']}\n"
        f"Total Faces Found: {stats['total_faces_found']}\n"
        f"Images with Faces: {stats['images_with_faces']}\n"
        f"Images without Faces: {stats['images_without_faces']}\n"
        f"Total Embeddings Uploaded: {stats['total_embeddings_uploaded']}\n"
        f"Total Batches Archived: {stats['total_batches_archived']}\n"
    )
    print(message)


if __name__ == "__main__":
    main()

----

# file: utils.py
# directory: .

import os
import logging
import cv2
import requests
import torch
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from logging.handlers import RotatingFileHandler
from models import Base


def get_engine():
    DB_HOST = os.environ.get('DB_HOST')
    DB_NAME = os.environ.get('DB_NAME')
    DB_USER = os.environ.get('DB_USER')
    DB_PASSWORD = os.environ.get('DB_PASSWORD')

    DATABASE_URL = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}'
    engine = create_engine(DATABASE_URL, pool_size=20, max_overflow=0)
    return engine


def get_session_factory(engine):
    return sessionmaker(bind=engine)


def setup_database(engine):
    Base.metadata.create_all(engine)
    # Создаём расширение vector, если его нет

    with engine.connect() as conn:
        conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
        conn.commit()


def preprocess_image(image_path):
    """
    Loads and processes an image.
    """
    image = cv2.imread(image_path)
    if image is not None:
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        return rgb_image
    return None


def configure_thread_logging(logger_name, log_filename, log_level, log_output):
    logger = logging.getLogger(logger_name)
    logger.setLevel(log_level)

    formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')

    handlers = []
    if log_output in ('file', 'both'):
        # Ensure log directory exists
        os.makedirs(os.path.dirname(log_filename), exist_ok=True)
        file_handler = RotatingFileHandler(log_filename, maxBytes=10*1024*1024, backupCount=5)
        file_handler.setFormatter(formatter)
        handlers.append(file_handler)

    if log_output in ('console', 'both'):
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        handlers.append(console_handler)

    # Remove existing handlers
    logger.handlers = []
    for handler in handlers:
        logger.addHandler(handler)

    return logger


def download_image(image_url, local_path, logger, stats_collector):
    try:
        response = requests.get(image_url, timeout=10)
        response.raise_for_status()
        with open(local_path, 'wb') as f:
            f.write(response.content)
        logger.info(f"Downloaded image: {image_url}")
        stats_collector.increment_files_downloaded()
        return True
    except Exception as e:
        logger.error(f"Error downloading {image_url}: {e}")
        return False

def get_face_analysis_model(device):
    """
    Returns a FaceAnalysis model from insightface.app.
    """
    from insightface.app import FaceAnalysis
    app = FaceAnalysis(providers=['CUDAExecutionProvider'] if device.type == 'cuda' else ['CPUExecutionProvider'])
    app.prepare(ctx_id=0 if device.type == 'cuda' else -1)
    return app

def get_insightface_embedding(image, app):
    """
    Генерирует эмбеддинг с помощью FaceAnalysis из insightface.app.
    """
    faces = app.get(image)
    if faces:
        # Берем первое обнаруженное лицо
        face = faces[0]
        embedding = face.embedding.flatten()
        return embedding
    else:
        return None

def get_facenet_embedding(image, mtcnn, model, device):
    """
    Генерирует эмбеддинг с помощью MTCNN и InceptionResnetV1 из facenet_pytorch.
    """
    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    face = mtcnn(img_rgb)
    if face is not None:
        face = face.unsqueeze(0).to(device)
        with torch.no_grad():
            embedding = model(face).cpu().numpy().flatten()
        return embedding
    else:
        return None
----

# file: db_writer.py
# directory: .

import os
import threading
from urllib.parse import urlparse

from utils import get_session_factory, configure_thread_logging
from models import HostLog, Checkpoint, BaseImageUrl, Image, Batch, BatchImage, ArchivedImage
import socket
import traceback
import config  # Импортируем модуль конфигурации

def db_writer_thread(db_queue, batch_ready_queue, engine, stats_collector, log_level, log_output):
    # Set up logger
    log_filename = f'logs/db_writer/db_writer_{config.MACHINE_ID}.log'
    db_writer_logger = configure_thread_logging('db_writer', log_filename, log_level, log_output)

    SessionFactory = get_session_factory(engine)
    session = SessionFactory()
    host_name = socket.gethostname()

    # Проверка существования HostLog
    existing_host_log = session.query(HostLog).filter_by(host_name=host_name, function_name='db_writer', log_file=log_filename).first()
    if not existing_host_log:
        host_log = HostLog(host_name=host_name, function_name='db_writer', log_file=log_filename)
        session.add(host_log)
        session.commit()
    else:
        host_log = existing_host_log

    while True:
        db_task = db_queue.get()
        if db_task is None:
            db_queue.task_done()
            break  # Termination signal

        task_type, data = db_task

        try:
            if task_type == 'store_batch':
                page_number, image_urls = data
                # Обработка чекпоинта
                page_url = f"http://camvideos.me/?page={page_number}"
                existing_checkpoint = session.query(Checkpoint).filter_by(page_url=page_url).first()
                if existing_checkpoint:
                    db_writer_logger.info(f"Page {page_url} already processed. Skipping.")
                    db_queue.task_done()
                    batch_ready_queue.put(None)
                    continue
                new_checkpoint = Checkpoint(page_url=page_url)
                session.add(new_checkpoint)
                session.commit()

                # Создание base_url
                base_urls = list(set([f"{urlparse(url).scheme}://{urlparse(url).netloc}" for url in image_urls]))
                base_url_str = base_urls[0]  # Предполагаем, что все изображения имеют одинаковый base_url
                base_url = BaseImageUrl(base_url=base_url_str)
                session.add(base_url)
                session.commit()

                # Создание батча
                batch = Batch(page_number=page_number)
                session.add(batch)
                session.commit()

                # Подготовка данных для вставки
                images_data = []
                image_filenames = []
                image_paths = []  # Новое
                image_urls_list = []

                for img_url in image_urls:
                    parsed_url = urlparse(img_url)
                    filename = os.path.basename(parsed_url.path)
                    path = os.path.dirname(parsed_url.path).lstrip('/')  # Extract the path
                    image = Image(base_url_id=base_url.id, path=path, filename=filename)
                    images_data.append(image)
                    image_filenames.append(filename)
                    image_paths.append(path)
                    image_urls_list.append(img_url)

                if not images_data:
                    db_writer_logger.info(f"No images to insert for page {page_url}.")
                    db_queue.task_done()
                    batch_ready_queue.put(None)
                    continue

                # Вставка изображений в базу данных
                session.add_all(images_data)
                session.commit()

                # Создание записей в batch_images
                batch_images_data = [BatchImage(batch_id=batch.id, image_id=img.id) for img in images_data]
                session.bulk_save_objects(batch_images_data)
                session.commit()

                # Передача информации о батче
                batch_info = {
                    'batch_id': batch.id,
                    'batch_dir': f"batch_{batch.id}",
                    'image_ids': [img.id for img in images_data],
                    'filenames': image_filenames,
                    'paths': image_paths,  # Новое
                    'image_urls': image_urls_list,
                }
                db_queue.task_done()
                db_writer_logger.info(f"Batch {batch.id} is ready and passed to downloader_thread.")
                batch_ready_queue.put(batch_info)
            elif task_type == 'mark_batch_processed':
                batch_id = data
                batch = session.query(Batch).filter_by(id=batch_id).first()
                if batch:
                    batch.processed = True
                    session.commit()
                db_queue.task_done()
        except Exception as e:
            session.rollback()
            db_writer_logger.error(f"Error in db_writer_thread: {e}")
            db_writer_logger.debug(traceback.format_exc())
            db_queue.task_done()
            batch_ready_queue.put(None)
    session.close()

----

# file: stats_collector.py
# directory: .
import json
import logging
import os
import threading
import time
from logging.handlers import RotatingFileHandler
import config  # Импортируем модуль конфигурации


class StatsCollector:
    def __init__(self):
        self.lock = threading.Lock()
        # Resettable stats
        self.files_downloaded = 0
        self.faces_found = 0
        self.embeddings_uploaded = 0
        self.embeddings_uploaded_by_type = {'embedding': 0, 'insightface_embedding': 0, 'both': 0}
        self.batch_processing_times = {'downloader': [], 'embedding_processor': [], 'embeddings_writer': [], 'archiver': []}
        self.last_reset_time = time.time()
        # Global stats
        self.total_files_downloaded = 0
        self.total_faces_found = 0
        self.total_embeddings_uploaded = 0
        self.total_images_processed = 0
        self.total_batches_processed = 0
        self.images_with_faces = 0
        self.images_without_faces = 0
        self.total_batches_archived = 0
        self.batches_processed_by_processor = 0
        self.start_time = time.time()

    def increment_embeddings_uploaded(self, count=1, embedding_type='both'):
        with self.lock:
            self.embeddings_uploaded += count
            self.total_embeddings_uploaded += count
            self.embeddings_uploaded_by_type[embedding_type] += count

    def increment_batches_processed_by_processor(self, count=1):
        with self.lock:
            self.batches_processed_by_processor += count

    def increment_files_downloaded(self, count=1):
        with self.lock:
            self.files_downloaded += count
            self.total_files_downloaded += count

    def increment_faces_found(self, count=1):
        with self.lock:
            self.faces_found += count
            self.total_faces_found += count

    def increment_images_processed(self, count=1):
        with self.lock:
            self.total_images_processed += count

    def increment_batches_processed(self, count=1):
        with self.lock:
            self.total_batches_processed += count

    def increment_images_with_faces(self, count=1):
        with self.lock:
            self.images_with_faces += count

    def increment_images_without_faces(self, count=1):
        with self.lock:
            self.images_without_faces += count

    def increment_batches_archived(self, count=1):
        with self.lock:
            self.total_batches_archived += count

    def add_batch_processing_time(self, thread_name, processing_time):
        with self.lock:
            self.batch_processing_times[thread_name].append(processing_time)

    def reset(self):
        with self.lock:
            stats = {
                # Interval stats
                'files_downloaded': self.files_downloaded,
                'faces_found': self.faces_found,
                'embeddings_uploaded': self.embeddings_uploaded,
                'embeddings_uploaded_by_type': self.embeddings_uploaded_by_type.copy(),
                'batch_processing_times': self.batch_processing_times.copy(),
                'elapsed_time': time.time() - self.last_reset_time,
                # Global stats
                'batches_processed_by_processor': self.batches_processed_by_processor,
                'total_files_downloaded': self.total_files_downloaded,
                'total_faces_found': self.total_faces_found,
                'total_embeddings_uploaded': self.total_embeddings_uploaded,
                'total_images_processed': self.total_images_processed,
                'total_batches_processed': self.total_batches_processed,
                'images_with_faces': self.images_with_faces,
                'images_without_faces': self.images_without_faces,
                'total_batches_archived': self.total_batches_archived,
                'start_time': self.start_time,
            }
            # Reset interval stats
            self.files_downloaded = 0
            self.faces_found = 0
            self.embeddings_uploaded = 0
            self.embeddings_uploaded_by_type = {'embedding': 0, 'insightface_embedding': 0, 'both': 0}
            self.batch_processing_times = {key: [] for key in self.batch_processing_times}
            self.last_reset_time = time.time()
        return stats

# Function to configure logging for a thread
def configure_thread_logging(logger_name, log_filename, log_level, log_output):
    logger = logging.getLogger(logger_name)
    logger.setLevel(log_level)

    formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')

    handlers = []
    if log_output in ('file', 'both'):
        # Ensure log directory exists
        os.makedirs(os.path.dirname(log_filename), exist_ok=True)
        file_handler = RotatingFileHandler(log_filename, maxBytes=10*1024*1024, backupCount=5)
        file_handler.setFormatter(formatter)
        handlers.append(file_handler)

    if log_output in ('console', 'both'):
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        handlers.append(console_handler)

    # Remove existing handlers
    logger.handlers = []
    for handler in handlers:
        logger.addHandler(handler)

    return logger

# Statistics logging thread
def stats_logger_thread(stats_collector, interval, stop_event, log_level, log_output, total_pages_to_process, page_queue, batch_queue, embeddings_queue, db_queue):
    # global MACHINE_ID
    stats_logger = configure_thread_logging('stats_logger', f'logs/stats_logger/stats_logger_{config.MACHINE_ID}.log', log_level, log_output)

    while not stop_event.is_set():
        time.sleep(interval)
        stats = stats_collector.reset()
        # Calculate average processing times
        avg_times = {}
        for thread_name, times in stats['batch_processing_times'].items():
            if times:
                avg_times[thread_name] = sum(times) / len(times)
            else:
                avg_times[thread_name] = 0

        total_images_processed = stats['total_images_processed']
        total_faces_found = stats['total_faces_found']
        images_with_faces = stats['images_with_faces']
        images_without_faces = stats['images_without_faces']
        average_faces_per_image = total_faces_found / total_images_processed if total_images_processed > 0 else 0

        # Estimate remaining time
        elapsed_time = time.time() - stats['start_time']
        batches_processed = stats['total_batches_processed']
        pages_processed = batches_processed  # Assuming one batch per page
        pages_remaining = total_pages_to_process - pages_processed
        avg_page_time = (elapsed_time / pages_processed) if pages_processed > 0 else 0
        estimated_time_remaining = avg_page_time * pages_remaining

        message = (
            f"Total Elapsed Time: {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\n"
            f"Interval Stats:\n"
            f"  Files Downloaded: {stats['files_downloaded']}\n"
            f"  Faces Found: {stats['faces_found']}\n"
            f"  Embeddings Uploaded by Type:\n"
            f"    embedding: {stats['embeddings_uploaded_by_type']['embedding']}\n"
            f"    insightface_embedding: {stats['embeddings_uploaded_by_type']['insightface_embedding']}\n"
            f"    both: {stats['embeddings_uploaded_by_type']['both']}\n"            
            f"Average Batch Processing Times:\n"
        )
        for thread_name, avg_time in avg_times.items():
            message += f"  {thread_name}: {avg_time:.2f}s\n"

        message += (
            f"Global Stats:\n"
            f"  Total Files Downloaded: {stats['total_files_downloaded']}\n"
            f"  Total Images Processed: {total_images_processed}\n"
            f"  Total Faces Found: {total_faces_found}\n"
            f"  Images with Faces: {images_with_faces}\n"
            f"  Images without Faces: {images_without_faces}\n"
            f"  Average Faces per Image: {average_faces_per_image:.2f}\n"
            f"  Total Batches Processed: {batches_processed}\n"
            f"Queue Sizes:\n"
            f"  Page Queue: {page_queue.qsize()}\n"
            f"  Batch Queue: {batch_queue.qsize()}\n"
            f"  Embeddings Queue: {embeddings_queue.qsize()}\n"
            f"  DB Queue: {db_queue.qsize()}\n"
            f"Estimated Time Remaining: {time.strftime('%H:%M:%S', time.gmtime(estimated_time_remaining))}\n"
        )

        stats_logger.info(message)
        print(message)
        # Записываем статистику в файл
        with open('stats.json', 'w') as f:
            json.dump(stats, f)
----

# file: web_server.py
# directory: .
from flask import Flask, request, jsonify
from utils import configure_thread_logging

def start_web_server(page_queue, stats_collector, port, log_level, log_output):
    app = Flask(__name__)

    # Set up logger
    log_filename = 'logs/web_server/web_server.log'
    logger = configure_thread_logging('web_server', log_filename, log_level, log_output)

    @app.route('/process_page', methods=['POST'])
    def process_page():
        h = request.args.get('h', type=int)
        if h is None:
            return jsonify({'error': 'Parameter h is required'}), 400
        page_queue.put(h)
        return jsonify({'status': f'Page {h} queued for processing'}), 200

    @app.route('/process_pages', methods=['POST'])
    def process_pages():
        h = request.args.get('h', type=int)
        k = request.args.get('k', type=int)
        m = request.args.get('m', type=int)
        n = request.args.get('n', type=int)
        if h is None or k is None:
            return jsonify({'error': 'Parameters h and k are required'}), 400
        if m is not None and n is not None:
            for page_num in range(h, k+1):
                if (page_num % m) == n:
                    page_queue.put(page_num)
        else:
            for page_num in range(h, k+1):
                page_queue.put(page_num)
        return jsonify({'status': f'Pages {h} to {k} queued for processing'}), 200

    @app.route('/stats', methods=['GET'])
    def get_stats():
        stats = stats_collector.reset()
        return jsonify(stats), 200

    # Run the Flask app
    app.run(host='0.0.0.0', port=port, threaded=True)

----

# file: view_web_server.py
# directory: .
from flask import Flask, request, render_template, redirect
from werkzeug.utils import secure_filename
from PIL import Image as PILImage, UnidentifiedImageError
import pyheif
import logging
import os
import torch
import numpy as np
import cv2
from facenet_pytorch import MTCNN, InceptionResnetV1
from insightface.app import FaceAnalysis
from insightface.utils import face_align
from sqlalchemy.orm import sessionmaker, scoped_session
from models import ImageEmbedding, Image, BaseImageUrl, ArchivedImage
from sqlalchemy import create_engine, text

# Настройки приложения
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['FAVICON_FOLDER'] = 'favicon'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['FAVICON_FOLDER'], exist_ok=True)
# Настройки логирования
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Поддерживаемые форматы изображений
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'heic', 'webp'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def convert_to_rgb(image_path):
    try:
        extension = image_path.rsplit('.', 1)[1].lower()
        if extension == 'heic':
            heif_file = pyheif.read(image_path)
            img = PILImage.frombytes(
                heif_file.mode, heif_file.size, heif_file.data,
                "raw", heif_file.mode, heif_file.stride
            )
        else:
            img = PILImage.open(image_path)
        img = img.convert('RGB')
        return np.array(img)
    except (UnidentifiedImageError, ValueError, pyheif.error.HeifError) as e:
        logger.error(f"Не удалось определить формат изображения: {image_path}, ошибка: {e}")
        return None

# Инициализация моделей для эмбеддингов
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
logger.info(f"Используется устройство: {device}")

# Инициализируем MTCNN с keep_all=True для обнаружения всех лиц
mtcnn = MTCNN(keep_all=True, device=device)
# Инициализируем модель InceptionResnetV1 для получения эмбеддингов FaceNet
facenet_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)
# Инициализируем модель InsightFace для получения эмбеддингов InsightFace
app_insight = FaceAnalysis(allowed_modules=['detection', 'recognition'],
                   providers=['CUDAExecutionProvider'] if device.type == 'cuda' else ['CPUExecutionProvider'])
app_insight.prepare(ctx_id=0 if device.type == 'cuda' else -1, det_size=(640, 640))

# Подключение к базе данных
DB_HOST = os.environ.get('DB_HOST', 'localhost')
DB_NAME = os.environ.get('DB_NAME', 'mydatabase')
DB_USER = os.environ.get('DB_USER', 'myuser')
DB_PASSWORD = os.environ.get('DB_PASSWORD', 'mypassword')

DATABASE_URL = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}'
engine = create_engine(DATABASE_URL)
SessionFactory = sessionmaker(bind=engine)
Session = scoped_session(SessionFactory)

@app.route('/favicon.ico')
def favicon():
    return send_from_directory(app.config['FAVICON_FOLDER'], 'favicon.ico', mimetype='image/vnd.microsoft.icon')
@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST':
        if 'image' not in request.files:
            return redirect(request.url)
        file = request.files['image']
        if file.filename == '':
            return redirect(request.url)
        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            upload_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(upload_path)

            # Конвертируем изображение в RGB
            img_rgb = convert_to_rgb(upload_path)
            if img_rgb is None:
                return render_template('index.html', message="Не удалось загрузить изображение.")

            # Обработка загруженного изображения
            try:
                # Детектирование лиц и получение координат и ключевых точек с помощью MTCNN
                boxes, probs, landmarks = mtcnn.detect(img_rgb, landmarks=True)
                if boxes is None or len(boxes) == 0:
                    logger.info("На загруженном изображении не обнаружено лицо.")
                    return render_template('index.html', message="На загруженном изображении не обнаружено лицо.")

                # Используем первое обнаруженное лицо
                box = boxes[0]
                landmark = landmarks[0]

                # Преобразуем landmark в numpy-массив с типом float64
                landmark = np.array(landmark, dtype=np.float64)

                # Извлекаем лицо для InceptionResnetV1
                face_aligned = mtcnn.extract(img_rgb, [box], save_path=None)
                if face_aligned is None or len(face_aligned) == 0:
                    logger.error("Не удалось извлечь лицо.")
                    return render_template('index.html', message="Не удалось извлечь лицо.")

                face_tensor = face_aligned[0].unsqueeze(0).to(device)

                with torch.no_grad():
                    embedding_facenet = facenet_model(face_tensor).cpu().numpy()

                # Выравниваем лицо для InsightFace
                face_aligned_insight = face_align.norm_crop(img_rgb, landmark=landmark, image_size=112)

                # Конвертируем в BGR и uint8
                face_aligned_insight_bgr = cv2.cvtColor(face_aligned_insight, cv2.COLOR_RGB2BGR)
                face_aligned_insight_bgr = face_aligned_insight_bgr.astype('uint8')

                # Получаем эмбеддинг с помощью InsightFace
                embedding_insight = app_insight.models['recognition'].get_feat(face_aligned_insight_bgr)

                # Подготавливаем эмбеддинги для SQL-запроса
                embedding_facenet_vector = embedding_facenet.flatten()
                embedding_insight_vector = embedding_insight.flatten()

                embedding_facenet_vector_list = embedding_facenet_vector.tolist()
                embedding_insight_vector_list = embedding_insight_vector.tolist()

                embedding_facenet_list_str = ','.join(map(str, embedding_facenet_vector_list))
                embedding_facenet_sql = f'ARRAY[{embedding_facenet_list_str}]'

                embedding_insight_list_str = ','.join(map(str, embedding_insight_vector_list))
                embedding_insight_sql = f'ARRAY[{embedding_insight_list_str}]'

                with Session() as session:
                    # Проверяем, есть ли эмбеддинги в базе данных
                    count_embeddings = session.query(ImageEmbedding).count()
                    if count_embeddings == 0:
                        logger.info("В базе данных нет эмбеддингов.")
                        return render_template('index.html', message="В базе данных нет эмбеддингов.")

                # Выполняем SQL-запрос, используя оба эмбеддинга и усредняя расстояния
                sql_query = f"""
                    WITH distances AS (
                        SELECT
                            ie.image_id,
                            ie.filename,
                            COALESCE((ie.embedding <=> {embedding_facenet_sql}::vector), 2.0) AS facenet_distance,
                            COALESCE((ie.insightface_embedding <=> {embedding_insight_sql}::vector), 2.0) AS insightface_distance,
                            ((COALESCE((ie.embedding <=> {embedding_facenet_sql}::vector), 2.0) + COALESCE((ie.insightface_embedding <=> {embedding_insight_sql}::vector), 2.0)) / 2) AS avg_distance
                        FROM
                            image_embeddings ie
                    )
                    SELECT
                        d.image_id,
                        d.filename,
                        i.path,
                        d.facenet_distance,
                        d.insightface_distance,
                        d.avg_distance,
                        ai.archive_url,
                        biu.base_url
                    FROM
                        distances d
                    LEFT JOIN images i ON d.image_id = i.id
                    LEFT JOIN base_image_urls biu ON i.base_url_id = biu.id
                    LEFT JOIN archived_images ai ON d.image_id = ai.image_id
                    ORDER BY
                        d.avg_distance
                    LIMIT 10;
                """

                results = session.execute(text(sql_query)).mappings().all()

                # Получение URL изображений
                similar_images = []
                for row in results:
                    image_id = row['image_id']
                    filename = row['filename']
                    path = row['path']
                    archive_url = row['archive_url']
                    base_url = row['base_url']
                    facenet_distance = row['facenet_distance']
                    insightface_distance = row['insightface_distance']
                    avg_distance = row['avg_distance']

                    # Предпочитаем archive_url, если он есть
                    if archive_url:
                        image_url = archive_url
                    elif base_url:
                        if path:
                            image_url = f"{base_url}/{path}/{filename}"
                        else:
                            image_url = f"{base_url}/{filename}"
                    else:
                        logger.error(f"Не удалось получить URL для изображения с id {image_id}.")
                        continue

                    similar_images.append({
                        'image_url': image_url,
                        'facenet_distance': facenet_distance,
                        'insightface_distance': insightface_distance,
                        'avg_distance': avg_distance
                    })

                # Возвращаем результат
                return render_template('results.html', images=similar_images)
            except Exception as e:
                logger.error(f"Ошибка при обработке загруженного изображения: {e}", exc_info=True)
                return render_template('index.html', message="Произошла ошибка при обработке изображения.")
    return render_template('index.html')

if __name__ == '__main__':
    # Запуск приложения без перезагрузчика
    app.run(host='0.0.0.0', port=8070, debug=True, use_reloader=False)

----

# file: downloader.py
# directory: .

import os
import threading
import time
import requests
import shutil
from concurrent.futures import ThreadPoolExecutor, wait
from urllib.parse import urlparse

from bs4 import BeautifulSoup

from utils import configure_thread_logging, get_session_factory, get_engine
from models import ArchivedImage, Image, BatchImage, Batch
import config  # Import configuration module


def downloader_thread(page_queue, batch_queue, db_queue, batch_ready_queue, download_dir, download_threads,
                      stats_collector, log_level, log_output, archive_enabled, condition, MAX_BATCHES_ON_DISK):
    # Set up logger for this function
    log_filename = f'logs/downloader/downloader_{config.MACHINE_ID}.log'
    downloader_logger = configure_thread_logging('downloader', log_filename, log_level, log_output)

    SessionFactory = get_session_factory(get_engine())
    session = SessionFactory()

    # Create ThreadPoolExecutor once
    executor = ThreadPoolExecutor(max_workers=download_threads)

    while True:
        page_info = page_queue.get()
        if page_info is None:
            page_queue.task_done()
            break  # Termination signal

        try:
            start_time = time.time()
            page_number = page_info['page_number']
            search_query = page_info.get('query')

            if search_query:
                page_url = f"http://camvideos.me/search/{search_query}?page={page_number}"
            else:
                page_url = f"http://camvideos.me/?page={page_number}"

            image_urls = process_page(page_url, stats_collector, log_level, log_output)

            if not image_urls:
                downloader_logger.info(f"No images on page {page_url}")
                page_queue.task_done()
                continue

            # Send task to store batch in database
            db_queue.put(('store_batch', (page_number, image_urls)))

            # Wait until current_batches_on_disk < MAX_BATCHES_ON_DISK
            with condition:
                while config.current_batches_on_disk >= MAX_BATCHES_ON_DISK:
                    condition.wait()
                config.current_batches_on_disk += 1  # Increment the counter

            # Wait for batch_info from db_writer
            batch_info = batch_ready_queue.get()
            if batch_info is None:
                batch_ready_queue.task_done()
                downloader_logger.error(f"Failed to create batch for page {page_url}.")
                # Decrease counter of current batches on disk
                with condition:
                    config.current_batches_on_disk -= 1
                    condition.notify()
                page_queue.task_done()
                continue

            batch_id = batch_info['batch_id']
            image_ids = batch_info['image_ids']
            filenames = batch_info['filenames']
            image_urls = batch_info['image_urls']
            image_paths = batch_info['paths']  # Ensure 'paths' are included

            filename_to_id = dict(zip(filenames, image_ids))
            id_to_url = dict(zip(image_ids, image_urls))

            # Update batch_info with actual information
            batch_dir_name = f"batch_{batch_id}"
            batch_dir = os.path.join(download_dir, batch_dir_name)
            os.makedirs(batch_dir, exist_ok=True)
            batch_info['batch_dir'] = batch_dir

            # Map filenames to URLs
            filename_to_url = dict(zip(filenames, image_urls))

            # Optimize database query for ArchivedImage
            if archive_enabled:
                # Get all ArchivedImage for current batch by filenames in one query
                try:
                    archived_images = session.query(ArchivedImage).filter(ArchivedImage.filename.in_(filenames)).all()
                    archived_filenames = {img.filename for img in archived_images}
                    filename_to_archive_url = {img.filename: img.archive_url for img in archived_images}
                except Exception as e:
                    downloader_logger.error(f"Database error while querying ArchivedImage: {e}")
                    archived_filenames = set()
                    filename_to_archive_url = {}
            else:
                archived_filenames = set()
                filename_to_archive_url = {}

            images_to_download = []
            failed_downloads = []  # List to keep track of failed downloads

            for filename in filenames:
                if archive_enabled and filename in archived_filenames:
                    # Image is in archive
                    archive_url = filename_to_archive_url[filename]
                    local_path = os.path.join(batch_dir, filename)
                    success = download_image(archive_url, local_path, downloader_logger, stats_collector)
                    if not success:
                        # If download from archive failed, add to list for downloading from source
                        images_to_download.append(filename)
                else:
                    images_to_download.append(filename)

            # Now download images not found in archive or failed to download from archive
            if images_to_download:
                futures = []

                for filename in images_to_download:
                    img_url = filename_to_url[filename]
                    local_path = os.path.join(batch_dir, filename)
                    future = executor.submit(download_image, img_url, local_path, downloader_logger, stats_collector)
                    future.filename = filename  # Attach filename to future for later identification
                    futures.append(future)

                # Wait for all downloads to complete for this batch
                wait(futures)

                # Check download results
                # Check download results
                for future in futures:
                    try:
                        result = future.result()
                        if not result:
                            failed_downloads.append(future.filename)
                    except Exception as e:
                        downloader_logger.info(f"Exception raised during downloading {future.filename}: {e}",
                                                exc_info=True)
                        failed_downloads.append(future.filename)

            # Handle failed downloads
            if failed_downloads:
                downloader_logger.warning(f"Failed to download images: {failed_downloads}")
                # Remove image records from database for failed downloads
                try:
                    # Remove from BatchImage
                    session.query(BatchImage).filter(BatchImage.batch_id == batch_id,
                                                     BatchImage.image_id.in_(
                                                         [filename_to_id[fn] for fn in failed_downloads])
                                                     ).delete(synchronize_session=False)
                    # Remove from Image
                    session.query(Image).filter(Image.id.in_([filename_to_id[fn] for fn in failed_downloads])
                                                ).delete(synchronize_session=False)
                    session.commit()
                    downloader_logger.info(f"Removed failed images from database: {failed_downloads}")
                except Exception as e:
                    session.rollback()
                    downloader_logger.error(f"Error removing failed images from database: {e}", exc_info=True)
            else:
                downloader_logger.info(f"All images for batch {batch_id} downloaded successfully.")

            # Now check if there are any images left in the batch after removing failed downloads
            remaining_images = [fn for fn in filenames if fn not in failed_downloads]
            if not remaining_images:
                downloader_logger.warning(f"No images left in batch {batch_id} after removing failed downloads.")
                # Remove batch from database
                try:
                    # Remove from BatchImage
                    session.query(BatchImage).filter(BatchImage.batch_id == batch_id).delete(synchronize_session=False)
                    # Remove batch
                    session.query(Batch).filter(Batch.id == batch_id).delete(synchronize_session=False)
                    session.commit()
                    downloader_logger.info(f"Removed batch {batch_id} from database.")
                except Exception as e:
                    session.rollback()
                    downloader_logger.error(f"Error removing batch {batch_id} from database: {e}", exc_info=True)
                # Delete batch directory
                shutil.rmtree(batch_dir, ignore_errors=True)
                # Decrease counter of current batches on disk
                with condition:
                    config.current_batches_on_disk -= 1
                    condition.notify()
                # Inform the processor to skip this batch
                batch_ready_queue.task_done()
                # Since we are not adding to batch_queue, we need to task_done for batch_queue as well
                batch_queue.task_done()
                page_queue.task_done()
                continue
            else:    # Update batch_info to reflect remaining images
                batch_info['filenames'] = remaining_images
                batch_info['image_ids'] = [filename_to_id[fn] for fn in remaining_images]
                batch_info['image_urls'] = [filename_to_url[fn] for fn in remaining_images]
                batch_info['paths'] = [batch_info['paths'][filenames.index(fn)] for fn in remaining_images]

                # **Rebuild filename_to_id and filename_to_url mappings**
                filename_to_id = {fn: filename_to_id[fn] for fn in remaining_images}
                filename_to_url = {fn: filename_to_url[fn] for fn in remaining_images}

                # Now, when batch is ready and all images are downloaded, add it to processing queue
                batch_queue.put(batch_info)
                downloader_logger.info(f"Batch {batch_id} added to processing queue.")

                batch_ready_queue.task_done()

        except Exception as e:
            downloader_logger.error(f"Error processing page {page_number}: {e}", exc_info=True)
            # Delete downloaded files and directory in case of error
            if 'batch_dir' in locals():
                shutil.rmtree(batch_dir, ignore_errors=True)
            # Decrease counter of current batches on disk
            with condition:
                config.current_batches_on_disk -= 1
                condition.notify()
        finally:
            processing_time = time.time() - start_time
            stats_collector.add_batch_processing_time('downloader', processing_time)
            page_queue.task_done()

    # Wait for all tasks to complete before shutting down
    executor.shutdown(wait=True)
    session.close()


def process_page(page_url, stats_collector, log_level, log_output):
    # Set up logger for this function
    log_filename = f'logs/html_processor/html_processor_{config.MACHINE_ID}.log'
    html_logger = configure_thread_logging('html_processor', log_filename, log_level, log_output)

    html_logger.info(f"Processing page: {page_url}")
    try:
        response = requests.get(page_url, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        html_logger.error(f"Error loading page {page_url}: {e}")
        return []

    try:
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract image URLs
        posts = soup.find_all('div', class_='post-container')
        image_urls = [post.find('img')['src'].replace('.th', '') for post in posts if post.find('img')]

        html_logger.info(f"Found {len(image_urls)} images on page {page_url}")
        return image_urls
    except Exception as e:
        html_logger.error(f"Error parsing page {page_url}: {e}", exc_info=True)
        return []


def get_total_pages_for_query(query):
    url = f"http://camvideos.me/search/{query}"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"Error loading page {url}: {e}")
        return 0

    try:
        soup = BeautifulSoup(response.text, 'html.parser')

        # Look for element with class 'numberofpages'
        numberofpages_element = soup.find('a', class_='numberofpages')
        if numberofpages_element:
            total_pages_text = numberofpages_element.text.strip('..')
            try:
                total_pages = int(total_pages_text)
                return total_pages
            except ValueError:
                print(f"Could not parse total pages from text: {total_pages_text}")
                return 1
        else:
            # If element not found, check for posts
            posts = soup.find_all('div', class_='post-container')
            if posts:
                return 1
            else:
                return 0
    except Exception as e:
        print(f"Error parsing page {url}: {e}")
        return 0


def download_image(image_url, local_path, downloader_logger, stats_collector):
    try:
        response = requests.get(image_url, timeout=10)
        response.raise_for_status()
        with open(local_path, 'wb') as f:
            f.write(response.content)
        downloader_logger.info(f"Downloaded image: {image_url}")
        stats_collector.increment_files_downloaded()
        return True
    except requests.RequestException as e:
        downloader_logger.info(f"Error downloading {image_url}: {e}")
        return False
    except IOError as e:
        downloader_logger.error(f"Error saving image {image_url} to {local_path}: {e}")
        return False

----

# file: models.py
# directory: .

from sqlalchemy import (
    Column,
    Integer,
    String,
    Boolean,
    ForeignKey,
    DateTime,
    UniqueConstraint,
    Index,
)
from sqlalchemy.orm import relationship, declarative_base
from datetime import datetime
from pgvector.sqlalchemy import Vector

Base = declarative_base()

class BaseImageUrl(Base):
    __tablename__ = 'base_image_urls'
    id = Column(Integer, primary_key=True)
    base_url = Column(String, nullable=False)
    images = relationship("Image", back_populates="base_image_url")

    __table_args__ = (
        Index('idx_base_image_urls_base_url', 'base_url'),
    )


class Image(Base):
    __tablename__ = 'images'
    id = Column(Integer, primary_key=True)
    base_url_id = Column(Integer, ForeignKey('base_image_urls.id'), nullable=False)
    path = Column(String, nullable=False)  # Поле path должно быть не NULL
    filename = Column(String, nullable=False)
    processed = Column(Boolean, default=False)
    base_image_url = relationship("BaseImageUrl", back_populates="images")
    embeddings = relationship("ImageEmbedding", back_populates="image")
    archived_image = relationship("ArchivedImage", back_populates="image", uselist=False)

    __table_args__ = (
        UniqueConstraint('base_url_id', 'path', 'filename', name='_base_image_uc'),
        Index('idx_images_base_url_id', 'base_url_id'),
        Index('idx_images_processed', 'processed'),
        Index('idx_images_filename', 'filename'),
        Index('idx_images_path', 'path'),
    )

class Batch(Base):
    __tablename__ = 'batches'
    id = Column(Integer, primary_key=True)
    page_number = Column(Integer, nullable=False)
    created_at = Column(DateTime, default=datetime.now())
    processed = Column(Boolean, default=False)
    images = relationship("BatchImage", back_populates="batch")
    logs = relationship("BatchLog", back_populates="batch")

    __table_args__ = (
        UniqueConstraint('page_number', name='_page_number_uc'),
        Index('idx_batches_processed', 'processed'),
    )


class BatchImage(Base):
    __tablename__ = 'batch_images'
    batch_id = Column(Integer, ForeignKey('batches.id'), primary_key=True)
    image_id = Column(Integer, ForeignKey('images.id'), primary_key=True)
    batch = relationship("Batch", back_populates="images")
    image = relationship("Image")

    __table_args__ = (
        Index('idx_batch_images_batch_id', 'batch_id'),
        Index('idx_batch_images_image_id', 'image_id'),
    )


class Checkpoint(Base):
    __tablename__ = 'checkpoints'
    id = Column(Integer, primary_key=True)
    page_url = Column(String, unique=True, nullable=False)

    __table_args__ = (
        Index('idx_checkpoints_page_url', 'page_url'),
    )


class ImageEmbedding(Base):
    __tablename__ = 'image_embeddings'

    id = Column(Integer, primary_key=True)
    image_id = Column(Integer, ForeignKey('images.id'))
    filename = Column(String, nullable=False)
    embedding = Column(Vector(512), nullable=True)
    insightface_embedding = Column(Vector(512), nullable=True)  # Новое поле

    image = relationship("Image", back_populates="embeddings")


    __table_args__ = (
        Index('idx_filename', 'filename'),
        Index('idx_image_embeddings_image_id', 'image_id'),
        Index(
            'idx_image_embeddings_embedding',
            'embedding',
            postgresql_using='ivfflat',
            postgresql_with={"lists": 100},
            postgresql_ops={'embedding': 'vector_cosine_ops'}
        ),
        Index(
            'idx_image_embeddings_insightface_embedding',
            'insightface_embedding',
            postgresql_using='ivfflat',
            postgresql_with={"lists": 100},
            postgresql_ops={'insightface_embedding': 'vector_cosine_ops'}
        ),
    )


# New models for host and log data
class HostLog(Base):
    __tablename__ = 'host_logs'
    id = Column(Integer, primary_key=True)
    host_name = Column(String, nullable=False)
    function_name = Column(String, nullable=False)
    log_file = Column(String, nullable=False)
    batches = relationship("BatchLog", back_populates="host_log")

    __table_args__ = (
        Index('idx_host_logs_host_name', 'host_name'),
        Index('idx_host_logs_function_name', 'function_name'),
    )


class BatchLog(Base):
    __tablename__ = 'batch_logs'
    id = Column(Integer, primary_key=True)
    batch_id = Column(Integer, ForeignKey('batches.id'), nullable=False)
    host_log_id = Column(Integer, ForeignKey('host_logs.id'), nullable=False)
    host_log = relationship("HostLog", back_populates="batches")
    batch = relationship("Batch", back_populates="logs")

    __table_args__ = (
        Index('idx_batch_logs_batch_id', 'batch_id'),
    )


class ArchivedImage(Base):
    __tablename__ = 'archived_images'
    id = Column(Integer, primary_key=True)
    image_id = Column(Integer, ForeignKey('images.id'), nullable=False)  # Добавлено поле внешнего ключа
    filename = Column(String, index=True)
    archive_url = Column(String)
    image = relationship("Image", back_populates="archived_image")

    __table_args__ = (
        Index('idx_archived_images_archive_url', 'archive_url'),
    )


class TestEmbedding(Base):
    __tablename__ = 'test_embeddings'

    id = Column(Integer, primary_key=True)
    filename = Column(String, nullable=False)
    embedding = Column(Vector(512), nullable=False)

    __table_args__ = (
        Index('idx_test_embeddings_embedding',
              'embedding',
              postgresql_using='ivfflat',
              postgresql_with={"lists": 100},
              postgresql_ops={'embedding': 'vector_cosine_ops'}
              ),
    )
----

# file: archiver_functions.py
# directory: .

import os
import logging

import boto3
from azure.storage.blob import BlobServiceClient
import ftplib
import paramiko


def archive_batch_to_s3(batch_dir, archive_config, logger):
    s3_endpoint = archive_config.get('s3_endpoint')
    s3_access_key = archive_config.get('s3_access_key')
    s3_secret_key = archive_config.get('s3_secret_key')
    s3_bucket = archive_config.get('s3_bucket')

    session = boto3.session.Session()
    s3_client = session.client(
        service_name='s3',
        endpoint_url=s3_endpoint,
        aws_access_key_id=s3_access_key,
        aws_secret_access_key=s3_secret_key
    )

    archive_urls = {}
    for root, dirs, files in os.walk(batch_dir):
        for file in files:
            local_path = os.path.join(root, file)
            s3_key = f"{os.path.basename(batch_dir)}/{file}"
            try:
                s3_client.upload_file(local_path, s3_bucket, s3_key)
                archive_url = f"{s3_endpoint}/{s3_bucket}/{s3_key}"
                archive_urls[file] = archive_url
                logger.info(f"Uploaded {file} to S3 at {archive_url}")
            except Exception as e:
                logger.error(f"Failed to upload {file} to S3: {e}")
    return archive_urls


def archive_batch_to_azure_blob(batch_dir, archive_config, logger):
    connection_string = archive_config.get('azure_connection_string')
    container_name = archive_config.get('azure_container_name')

    blob_service_client = BlobServiceClient.from_connection_string(connection_string)
    container_client = blob_service_client.get_container_client(container_name)

    archive_urls = {}
    for root, dirs, files in os.walk(batch_dir):
        for file in files:
            local_path = os.path.join(root, file)
            blob_name = f"{os.path.basename(batch_dir)}/{file}"
            try:
                with open(local_path, "rb") as data:
                    container_client.upload_blob(name=blob_name, data=data)
                archive_url = f"https://{blob_service_client.account_name}.blob.core.windows.net/{container_name}/{blob_name}"
                archive_urls[file] = archive_url
                logger.info(f"Uploaded {file} to Azure Blob at {archive_url}")
            except Exception as e:
                logger.error(f"Failed to upload {file} to Azure Blob: {e}")
    return archive_urls


def archive_batch_to_ftp(batch_dir, archive_config, logger):
    ftp_host = archive_config.get('ftp_host')
    ftp_port = archive_config.get('ftp_port', 21)
    ftp_username = archive_config.get('ftp_username')
    ftp_password = archive_config.get('ftp_password')
    ftp_directory = archive_config.get('ftp_directory', '/')

    archive_urls = {}
    try:
        ftp = ftplib.FTP()
        ftp.connect(ftp_host, ftp_port)
        ftp.login(ftp_username, ftp_password)
        ftp.cwd(ftp_directory)

        for root, dirs, files in os.walk(batch_dir):
            for file in files:
                local_path = os.path.join(root, file)
                with open(local_path, 'rb') as f:
                    ftp.storbinary(f'STOR {file}', f)
                archive_url = f"ftp://{ftp_host}{ftp_directory}/{file}"
                archive_urls[file] = archive_url
                logger.info(f"Uploaded {file} to FTP at {archive_url}")
        ftp.quit()
    except Exception as e:
        logger.error(f"Failed to upload files to FTP: {e}")
    return archive_urls


def archive_batch_to_sftp(batch_dir, archive_config, logger):
    sftp_host = archive_config.get('sftp_host')
    sftp_port = archive_config.get('sftp_port', 22)
    sftp_username = archive_config.get('sftp_username')
    sftp_password = archive_config.get('sftp_password')
    sftp_directory = archive_config.get('sftp_directory', '/')

    archive_urls = {}
    try:
        transport = paramiko.Transport((sftp_host, sftp_port))
        transport.connect(username=sftp_username, password=sftp_password)
        sftp = paramiko.SFTPClient.from_transport(transport)

        # Ensure the directory exists
        try:
            sftp.chdir(sftp_directory)
        except IOError:
            sftp.mkdir(sftp_directory)
            sftp.chdir(sftp_directory)

        for root, dirs, files in os.walk(batch_dir):
            for file in files:
                local_path = os.path.join(root, file)
                remote_path = f"{sftp_directory}/{os.path.basename(batch_dir)}/{file}"
                # Ensure remote directory exists
                try:
                    sftp.stat(os.path.dirname(remote_path))
                except IOError:
                    sftp.mkdir(os.path.dirname(remote_path))
                sftp.put(local_path, remote_path)
                archive_url = f"sftp://{sftp_host}{remote_path}"
                archive_urls[file] = archive_url
                logger.info(f"Uploaded {file} to SFTP at {archive_url}")
        sftp.close()
        transport.close()
    except Exception as e:
        logger.error(f"Failed to upload files to SFTP: {e}")
    return archive_urls
